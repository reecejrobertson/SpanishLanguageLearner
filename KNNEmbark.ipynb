{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b275c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_cleaning\n",
    "from scipy import sparse\n",
    "from load_data import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78e4aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>concept_id</th>\n",
       "      <th>prioritized</th>\n",
       "      <th>updated_timestamp</th>\n",
       "      <th>starred</th>\n",
       "      <th>mistaken</th>\n",
       "      <th>words_studied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314379</th>\n",
       "      <td>3526582993919929</td>\n",
       "      <td>4563</td>\n",
       "      <td>0</td>\n",
       "      <td>1574452735189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614448</th>\n",
       "      <td>3685663292268160</td>\n",
       "      <td>4032</td>\n",
       "      <td>2</td>\n",
       "      <td>1619198345660</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000669</th>\n",
       "      <td>3597330111278142</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>1611811146354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600483</th>\n",
       "      <td>3684408494931459</td>\n",
       "      <td>3998</td>\n",
       "      <td>0</td>\n",
       "      <td>1606009155929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869571</th>\n",
       "      <td>3561325693434876</td>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>1595094766005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179535</th>\n",
       "      <td>3600827686694396</td>\n",
       "      <td>1984</td>\n",
       "      <td>0</td>\n",
       "      <td>1622646771303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517916</th>\n",
       "      <td>3632697689483654</td>\n",
       "      <td>5207</td>\n",
       "      <td>0</td>\n",
       "      <td>1588453866929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289971</th>\n",
       "      <td>3602810761377396</td>\n",
       "      <td>3969</td>\n",
       "      <td>2</td>\n",
       "      <td>1599331527222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792021</th>\n",
       "      <td>3557000681978879</td>\n",
       "      <td>5915</td>\n",
       "      <td>0</td>\n",
       "      <td>1599744826181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5142041</th>\n",
       "      <td>3735379835132428</td>\n",
       "      <td>5049</td>\n",
       "      <td>0</td>\n",
       "      <td>1618424410279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id  concept_id  prioritized  updated_timestamp  \\\n",
       "314379   3526582993919929        4563            0      1574452735189   \n",
       "4614448  3685663292268160        4032            2      1619198345660   \n",
       "2000669  3597330111278142        1968            0      1611811146354   \n",
       "4600483  3684408494931459        3998            0      1606009155929   \n",
       "869571   3561325693434876         358            0      1595094766005   \n",
       "2179535  3600827686694396        1984            0      1622646771303   \n",
       "3517916  3632697689483654        5207            0      1588453866929   \n",
       "2289971  3602810761377396        3969            2      1599331527222   \n",
       "792021   3557000681978879        5915            0      1599744826181   \n",
       "5142041  3735379835132428        5049            0      1618424410279   \n",
       "\n",
       "         starred  mistaken  words_studied  \n",
       "314379         0         0            508  \n",
       "4614448        1         0            736  \n",
       "2000669        0         0            142  \n",
       "4600483        0         0             61  \n",
       "869571         0         0            597  \n",
       "2179535        0         0             39  \n",
       "3517916        0         0            539  \n",
       "2289971        1         0            427  \n",
       "792021         0         0             24  \n",
       "5142041        0         0            626  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get the data created by the data cleaning script\n",
    "# List of words encountered by the user on the app.  If a word isn't listed for a certain user,\n",
    "# then that user hasn't yet encountered that word in the app.\n",
    "database = pd.read_csv(\"Vol3CleanedData.csv\", index_col = 0)\n",
    "# database, but limited based on if a user has starred at least one word.\n",
    "users_who_star = pd.read_csv(\"Vol3StarredData.csv\", index_col=0)\n",
    "# Cast boolean columns as integers\n",
    "users_who_star['starred'] = users_who_star['starred'].astype('int')\n",
    "users_who_star['mistaken'] = users_who_star['mistaken'].astype('int')\n",
    "\n",
    "display(users_who_star.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0c0ee",
   "metadata": {},
   "source": [
    "### Trial 1: Naive implementation of encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70f5fd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6085, 2424)\n",
      "Time: 87.22972822189331\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#One-hot encode the words to get a sparse matrix, and combine based on user_id\n",
    "df = users_who_star.copy()\n",
    "# starred = df['starred'].to_numpy()\n",
    "# mistaken = df['mistaken'].to_numpy()\n",
    "# words.drop(columns=['updated_timestamp','words_studied','prioritized','starred','mistaken'], inplace=True)\n",
    "# print(len(words['concept_id'].unique()))\n",
    "# indices = words['user_id']\n",
    "\n",
    "column_labels = df['concept_id'].unique()\n",
    "row_labels = df['user_id'].unique()\n",
    "#Initialize Sparse Matrix\n",
    "# Layer learned is a T/F (0/1) indicator if the word has been learned at all\n",
    "# Layer starred is a T/F (0/1) indicator if the word has been starred or mistaken\n",
    "row_index = []\n",
    "col_index = []\n",
    "learned_data = []\n",
    "starred_data = []\n",
    "for index, row in df.iterrows():\n",
    "    i = np.where(row_labels == row['user_id'])[0][0]\n",
    "    j = np.where(column_labels == row['concept_id'])[0][0]\n",
    "    #Add indices and data to lists to construct COO Sparse matrix\n",
    "    row_index.append(i)\n",
    "    col_index.append(j)\n",
    "    learned_data.append(1)\n",
    "    starred_data.append(max((row['starred'], row['mistaken'])))\n",
    "\n",
    "learned = sparse.coo_matrix((learned_data,(row_index,col_index)))\n",
    "starred = sparse.coo_matrix((starred_data,(row_index,col_index)))\n",
    "\n",
    "print(learned.shape)\n",
    "print(\"Time:\", time.time()-start,\"seconds\")\n",
    "### Forgive me!  I tried to do it in a smart way, but I couldn't get it to work.\n",
    "### As such, I defaulted to the Naive way.  If someone wants to attempt, \n",
    "### Commented throughout this cell are various attempts and functions that might\n",
    "### prove useful.\n",
    "    \n",
    "# list_words = words.groupby('user_id')['concept_id'].apply(list)\n",
    "# df = pd.DataFrame(list_words)\n",
    "# data3 = df['user_id'].apply(collections.Counter)\n",
    "# pd.DataFrame.from_records(data3).fillna(value=0)\n",
    "# words = words.groupby('user_id').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd52ea",
   "metadata": {},
   "source": [
    "### Trial 2: NumPy Implementation Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "132c6704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6085, 2424, 2)\n",
      "Time: 81.62009239196777 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#One-hot encode the words to get a sparse matrix, and combine based on user_id\n",
    "df = users_who_star.copy()\n",
    "# starred = df['starred'].to_numpy()\n",
    "# mistaken = df['mistaken'].to_numpy()\n",
    "# words.drop(columns=['updated_timestamp','words_studied','prioritized','starred','mistaken'], inplace=True)\n",
    "# print(len(words['concept_id'].unique()))\n",
    "# indices = words['user_id']\n",
    "\n",
    "column_labels = df['concept_id'].unique()\n",
    "row_labels = df['user_id'].unique()\n",
    "#Initialize Matrix\n",
    "# Layer 0 is a T/F (0/1) indicator if the word has been learned at all\n",
    "# Layer 1 is a T/F (0/1) indicator if the word has been starred or mistaken\n",
    "data = np.zeros((len(row_labels),len(column_labels),2))\n",
    "for index, row in df.iterrows():\n",
    "    i = np.where(row_labels == row['user_id'])[0][0]\n",
    "    j = np.where(column_labels == row['concept_id'])[0][0]\n",
    "    #Add indices and data to lists to construct matrix\n",
    "    data[i,j,0] = 1\n",
    "    data[i,j,1] = max((row['starred'], row['mistaken']))\n",
    "\n",
    "print(data.shape)\n",
    "print(\"Time:\", time.time()-start,\"seconds\")\n",
    "### Forgive me!  I tried to do it in a smart way, but I couldn't get it to work.\n",
    "### As such, I defaulted to the Naive way.  If someone wants to attempt, \n",
    "### Commented throughout this cell are various attempts and functions that might\n",
    "### prove useful.\n",
    "    \n",
    "# list_words = words.groupby('user_id')['concept_id'].apply(list)\n",
    "# df = pd.DataFrame(list_words)\n",
    "# data3 = df['user_id'].apply(collections.Counter)\n",
    "# pd.DataFrame.from_records(data3).fillna(value=0)\n",
    "# words = words.groupby('user_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a80239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, test_size=.2):\n",
    "    n = X.shape[0]\n",
    "    test_size_n = int(test_size*n)\n",
    "    test_indices = np.random.choice(n,test_size_n,replace=False)\n",
    "    X_train = np.delete(X, test_indices, 0)\n",
    "    test_users = X[test_indices,:,:]\n",
    "    #We must selectively identify words to exclude from the learned words set, in order to\n",
    "    #be able to usefully judge whether suggested words were actually starred.\n",
    "    #Must also find balance between users who starred few words versus users who star all words\n",
    "    for i in range(len(test_users)):\n",
    "        starred = test_users[i,:,1]\n",
    "        learned = test_users[i,:,0]\n",
    "        num_starred = float(np.count_nonzero(starred))\n",
    "        ratio = num_starred/np.count_nonzero(learned)\n",
    "        test_rate = .75 if ratio >= .75 else 1\n",
    "        test_indices = np.random.choice(len(starred),int(test_rate*num_starred),replace=False, p=starred/num_starred)\n",
    "        #Delete selected words from learned vocabulary,\n",
    "        #Test to see if predictions include these words\n",
    "        test_users[i,test_indices,0] = 0\n",
    "        test_users[i,:,1] = 0\n",
    "        test_users[i,test_indices,1] = 1\n",
    "        \n",
    "    return X_train, test_users[:,:,0], test_users[:,:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3738c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use that matrix to determine how close a new datapoint is, and suggest words that are in the set-minus\n",
    "\n",
    "# Another possible way (higher computational complexity):\n",
    "# Make lists of each user with words in order, to find new words that were learned later (excludes all previous words)\n",
    "# Use percentage of the list matches with each user to determine \"closeness\"\n",
    "\n",
    "class KNNPredictor():\n",
    "    '''\n",
    "    Predictor to predict what data points are closest (see predict function for algorithm),\n",
    "    and suggests new words that are in the set subtraction\n",
    "    \n",
    "    Note that this KNN is unconventional, as it predicts values for all other features based on the inputted features.\n",
    "    '''\n",
    "    def __init__(self,k,column_labels):\n",
    "        self.column_labels = column_labels\n",
    "        self.k=k\n",
    "    \n",
    "    def fit(self,X):\n",
    "        '''\n",
    "        Params:\n",
    "            X (ndarray(m,n,2)): matrix encoded with all possible words from all data points.  \n",
    "        '''\n",
    "        #Add data to the class\n",
    "        self.X = X\n",
    "        return self\n",
    "    \n",
    "    def predict(self,x,num_suggest=5, random_sample = False):\n",
    "        '''\n",
    "        Predicts which words will be starred or mistaken in the future.  \n",
    "        Params:\n",
    "            x (ndarray(k,l,2)): (possibly batch) ndarray encoded with all possible words of samples to predict on.  \n",
    "        '''\n",
    "        # Subtract row x from all the rows of self.X, and count the number of -1's.\n",
    "        # The argmin of counting the -1's is the closest to x\n",
    "        # e.g. -1's when x has a word that X[i] doesn't, meaning they are less similar.\n",
    "        # Then use the remaining +1's to pick a word to suggest\n",
    "        # e.g. 1's when X[i] has a word that x doesn't.\n",
    "        # If choosing k>1 neighbors, sum up the columns and pick a random word, weighted based on frequency\n",
    "        samples = x.copy()\n",
    "        predictions = np.zeros((len(samples),num_suggest))\n",
    "        for i, row in enumerate(samples):\n",
    "            #Use algorithm above to determine 'closeness'\n",
    "            distances = self.X[:,:,0] - row\n",
    "            negative_mask = distances < 0\n",
    "            closeness = np.count_nonzero(negative_mask, axis=1)\n",
    "            #Find indices of the k closest users\n",
    "            indices = closeness.argsort()[:self.k]\n",
    "            \n",
    "            #Construct probability distribution and sample without replacement\n",
    "            #Suggest only words that the user in question has not already learned\n",
    "            possible_suggest = self.X[indices,:,1] - row\n",
    "            possible_suggest = (possible_suggest > 0).astype(int)\n",
    "            distribution = possible_suggest.sum(axis=0)\n",
    "            distribution = distribution/np.sum(distribution)\n",
    "            if random_sample:\n",
    "                #If random is wanted, samples are drawn from a multinomial distribution\n",
    "                suggest = np.random.choice(len(distribution),num_suggest,False,distribution)\n",
    "            else:\n",
    "                #If not random, then the n highest frequency words are suggested.\n",
    "                suggest = (-distribution).argsort()[:num_suggest]\n",
    "            \n",
    "            #Insert predicted values to output matrix\n",
    "            predictions[i,:] = self.column_labels[suggest]\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def score(self,y_true, pred):\n",
    "        # This score is used to verify if the predicted words are in the set of actually starred words.\n",
    "        # Returns number of correctly identified over number of total suggested\n",
    "        # NOTE: This is perhaps an unreliable score.  It is possible that a word suggested to a user\n",
    "        # would be useful in the future, and might not be contained in the already-starred words.\n",
    "        accuracy = np.zeros(len(pred))\n",
    "        for i in range(len(pred)):\n",
    "            n = pred.shape[1]\n",
    "            num_correct = 0\n",
    "            for word in pred[i]:\n",
    "                index = np.where(self.column_labels == word)\n",
    "                if y_true[i,index] == 1:\n",
    "                    num_correct += 1\n",
    "            accuracy[i] = float(num_correct)/n\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0547e3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.008158206939697266\n",
      "Prediction time for 100 users: 8.006340026855469\n",
      "Accuracy: 0.1958762886597938\n",
      "[0.8, 0.4, 0.0, 0.2, 0.0, 0.4, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.4, 0.6, 0.6, 0.2, 0.0, 0.6, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.2, 0.2, 0.4, 0.0, 0.2, 0.0, 0.0, 1.0, 0.2, 0.0, 0.2, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.8, 0.2, 0.6, 0.4, 1.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.2, 0.0, 0.8, 0.2, 0.0, 0.0, 0.4, 0.2, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "\n",
    "X_train, tups = train_test_split(test_size=100)\n",
    "column_labels, X_train = encode(X_train,expanded = False)\n",
    "start = time.time()\n",
    "knn = KNNPredictor(5,column_labels).fit(X_train)\n",
    "print(\"Train time:\", time.time()-start)\n",
    "start = time.time()\n",
    "scores = []\n",
    "for tup in tups:\n",
    "    _, X_test = encode(tup[0],expanded = False,column_labels=column_labels)\n",
    "    _, y_test = encode(tup[1],expanded = False,column_labels=column_labels)\n",
    "    preds = knn.predict(X_test[:,:,0])\n",
    "    score = knn.score(y_test[:,:,0], preds)\n",
    "    if len(score) > 0:\n",
    "        scores.append(score.item())\n",
    "print(\"Prediction time for\",len(tups),\"users:\", time.time()-start)\n",
    "print(\"Accuracy:\",np.mean(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1516826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 0.007532596588134766\n",
      "Prediction time for 1 users: 0.09897923469543457\n",
      "Accuracy: 0.0\n",
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "\n",
    "X_train, tups = train_test_split(test_size=1)\n",
    "column_labels, X_train = encode(X_train,expanded = False)\n",
    "start = time.time()\n",
    "knn = KNNPredictor(5,column_labels).fit(X_train)\n",
    "print(\"Train time:\", time.time()-start)\n",
    "start = time.time()\n",
    "scores = []\n",
    "for tup in tups:\n",
    "    _, X_test = encode(tup[0],expanded = False,column_labels=column_labels)\n",
    "    _, y_test = encode(tup[1],expanded = False,column_labels=column_labels)\n",
    "    preds = knn.predict(X_test[:,:,0])\n",
    "    scores.append(knn.score(y_test[:,:,1], preds)[0])\n",
    "print(\"Prediction time for\",len(tups),\"users:\", time.time()-start)\n",
    "print(\"Accuracy:\",np.mean(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6872d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
